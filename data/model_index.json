{
  "openai-community/gpt2": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "tflite",
      "rust",
      "onnx",
      "safetensors",
      "gpt2",
      "text-generation",
      "exbert",
      "en",
      "doi:10.57967/hf/0039",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "text-generation-inference",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://huggingface.co/openai-community/gpt2",
    "required_classes": {
      "model": "AutoModelForCausalLM",
      "tokenizer": "AutoTokenizer"
    }
  },
  "cardiffnlp/twitter-roberta-base-sentiment-latest": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "transformers"
    ],
    "pipeline_tag": "sentiment-analysis",
    "model_card_url": "https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest",
    "required_classes": {
      "model": "AutoModelForSequenceClassification",
      "tokenizer": "AutoTokenizer"
    }
  },
  "yolov8n": {
    "is_online": false,
    "model_source": "ultralytics",
    "model_class": "UltralyticsModel",
    "tags": [
      "yolo",
      "object-detection",
      "pytorch",
      "yolov8",
      "yolov8n",
      "computer-vision",
      "en",
      "license:agpl-3.0",
      "region:us"
    ],
    "model_desc": "Ultralytics YOLOv8 is a cutting-edge, state-of-the-art (SOTA) model designed for fast, accurate, and easy-to-use object detection. It builds upon the success of previous YOLO versions and introduces new features and improvements to further enhance performance and flexibility. YOLOv8 is suitable for a wide range of tasks including object detection, instance segmentation, and image classification. YOLOv8n is the smallest version of YOLOv8 series with 3.2M params.",
    "model_detail": "https://docs.ultralytics.com/models/yolov8/"
  },
  "yolov8s": {
    "is_online": false,
    "model_source": "ultralytics",
    "model_class": "UltralyticsModel",
    "tags": [
      "yolo",
      "object-detection",
      "pytorch",
      "yolov8",
      "yolov8s",
      "computer-vision",
      "en",
      "license:agpl-3.0"
    ],
    "model_desc": "Ultralytics YOLOv8 is a cutting-edge, state-of-the-art (SOTA) model designed for fast, accurate, and easy-to-use object detection. It builds upon the success of previous YOLO versions and introduces new features and improvements to further enhance performance and flexibility. YOLOv8 is suitable for a wide range of tasks including object detection, instance segmentation, and image classification. YOLOv8s has 11.2M params.",
    "model_detail": "https://docs.ultralytics.com/models/yolov8/"
  },
  "yolov10n": {
    "is_online": false,
    "model_source": "ultralytics",
    "model_class": "UltralyticsModel",
    "tags": [
      "yolo",
      "object-detection",
      "pytorch",
      "yolov10",
      "yolov10n",
      "computer-vision",
      "en",
      "license:agpl-3.0"
    ],
    "model_detail": "https://docs.ultralytics.com/models/yolov10/"
  },
  "yolov10s": {
    "is_online": false,
    "model_source": "ultralytics",
    "model_class": "UltralyticsModel",
    "tags": [
      "yolo",
      "object-detection",
      "pytorch",
      "yolov10",
      "yolov10s",
      "computer-vision",
      "en",
      "license:agpl-3.0"
    ],
    "model_detail": "https://docs.ultralytics.com/models/yolov10/"
  },
  "google/owlvit-base-patch32" : {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "transformers",
      "pytorch",
      "owlvit",
      "object-detection",
      "vision",
      "en",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "object-detection",
    "model_card_url": "https://huggingface.co/google/owlvit-base-patch32",
    "required_classes": {
      "model": "OwlViTForObjectDetection",
      "processor": "OwlViTProcessor"
    }
  },
  "meta-llama/Meta-Llama-3-8B": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "llama",
      "meta",
      "llama-3",
      "text-generation",
      "text-generation-inference",
      "en",
      "license:llama3",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://huggingface.co/meta-llama/Meta-Llama-3-8B",
    "required_classes": {
      "model": "AutoModelForCausalLM",
      "tokenizer": "AutoTokenizer"
    },
    "model_desc": "Meta-Llama-3-8B is part of the Meta Llama 3 family of large language models, optimized for dialogue use cases. It's an 8B parameter model that outperforms many available open-source chat models on common industry benchmarks.",
    "model_detail": "https://llama.meta.com/get-started/"
  },
  "nomic-ai/gpt4all-j": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "gptj",
      "text-generation",
      "en",
      "license:apache-2.0"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://huggingface.co/nomic-ai/gpt4all-j",
    "required_classes": {
      "model": "AutoModelForCausalLM",
      "tokenizer": "AutoTokenizer"
    },
    "model_desc": "GPT4All-J is an Apache-2 licensed chatbot trained over a massive curated corpus of assistant interactions, including word problems, multi-turn dialogue, code, poems, songs, and stories. It's a finetuned version of GPT-J optimized for assistant-style interactions.",
    "model_detail": "https://github.com/nomic-ai/gpt4all"
  },
  "TheBloke/Llama-2-13B-chat-GGML": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "transformers",
      "llama",
      "llama-2",
      "ggml",
      "text-generation",
      "chat",
      "en",
      "license:llama2"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML",
    "required_classes": {
      "model": "AutoModelForCausalLM",
      "tokenizer": "AutoTokenizer"
    },
    "model_desc": "Llama-2-13B-chat-GGML is a quantized version of the Llama 2 13B chat model, optimized for efficient inference on CPU using the GGML format. It's designed for natural language processing tasks, particularly suited for chat-based applications.",
    "model_detail": "https://github.com/ggerganov/llama.cpp"
  }
}