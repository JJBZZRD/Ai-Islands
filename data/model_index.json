{
  "openai-community/gpt2": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "tflite",
      "rust",
      "onnx",
      "safetensors",
      "gpt2",
      "text-generation",
      "exbert",
      "en",
      "doi:10.57967/hf/0039",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "text-generation-inference",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://huggingface.co/openai-community/gpt2",
    "requirements":{
      "required_classes":{
        "model": "AutoModelForCausalLM",
        "tokenizer": "AutoTokenizer"
      }
    }
  },
  "cardiffnlp/twitter-roberta-base-sentiment-latest": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "transformers"
    ],
    "pipeline_tag": "sentiment-analysis",
    "model_card_url": "https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest",
    "requirements":{
      "required_classes":{
        "model": "AutoModelForSequenceClassification",
        "tokenizer": "AutoTokenizer"
      }
    }
  },
  "yolov8n": {
    "is_online": false,
    "model_source": "ultralytics",
    "model_class": "UltralyticsModel",
    "tags": [
      "yolo",
      "object-detection",
      "pytorch",
      "yolov8",
      "yolov8n",
      "computer-vision",
      "en",
      "license:agpl-3.0",
      "region:us"
    ],
    "model_desc": "Ultralytics YOLOv8 is a cutting-edge, state-of-the-art (SOTA) model designed for fast, accurate, and easy-to-use object detection. It builds upon the success of previous YOLO versions and introduces new features and improvements to further enhance performance and flexibility. YOLOv8 is suitable for a wide range of tasks including object detection, instance segmentation, and image classification. YOLOv8n is the smallest version of YOLOv8 series with 3.2M params.",
    "model_detail": "https://docs.ultralytics.com/models/yolov8/"
  },
  "yolov8s": {
    "is_online": false,
    "model_source": "ultralytics",
    "model_class": "UltralyticsModel",
    "tags": [
      "yolo",
      "object-detection",
      "pytorch",
      "yolov8",
      "yolov8s",
      "computer-vision",
      "en",
      "license:agpl-3.0"
    ],
    "model_desc": "Ultralytics YOLOv8 is a cutting-edge, state-of-the-art (SOTA) model designed for fast, accurate, and easy-to-use object detection. It builds upon the success of previous YOLO versions and introduces new features and improvements to further enhance performance and flexibility. YOLOv8 is suitable for a wide range of tasks including object detection, instance segmentation, and image classification. YOLOv8s has 11.2M params.",
    "model_detail": "https://docs.ultralytics.com/models/yolov8/"
  },
  "yolov10n": {
    "is_online": false,
    "model_source": "ultralytics",
    "model_class": "UltralyticsModel",
    "tags": [
      "yolo",
      "object-detection",
      "pytorch",
      "yolov10",
      "yolov10n",
      "computer-vision",
      "en",
      "license:agpl-3.0"
    ],
    "model_detail": "https://docs.ultralytics.com/models/yolov10/"
  },
  "yolov10s": {
    "is_online": false,
    "model_source": "ultralytics",
    "model_class": "UltralyticsModel",
    "tags": [
      "yolo",
      "object-detection",
      "pytorch",
      "yolov10",
      "yolov10s",
      "computer-vision",
      "en",
      "license:agpl-3.0"
    ],
    "model_detail": "https://docs.ultralytics.com/models/yolov10/"
  },
  "google/owlvit-base-patch32" : {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "transformers",
      "pytorch",
      "owlvit",
      "object-detection",
      "vision",
      "en",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "object-detection",
    "model_card_url": "https://huggingface.co/google/owlvit-base-patch32",
    "requirements":{
      "required_classes":{
        "model": "OwlViTForObjectDetection",
        "processor": "OwlViTProcessor"
      }
    }
  },
  "meta-llama/Meta-Llama-3-8B": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "llama",
      "meta",
      "llama-3",
      "text-generation",
      "text-generation-inference",
      "en",
      "license:llama3",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://huggingface.co/meta-llama/Meta-Llama-3-8B",
    "requirements":{
      "required_classes":{
        "model": "AutoModelForCausalLM",
        "tokenizer": "AutoTokenizer"
      },
      "requires_auth": true
    },
    "model_desc": "Meta-Llama-3-8B is part of the Meta Llama 3 family of large language models, optimized for dialogue use cases. It's an 8B parameter model that outperforms many available open-source chat models on common industry benchmarks.",
    "model_detail": "https://llama.meta.com/get-started/"
  },
  "nomic-ai/gpt4all-j": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "gptj",
      "text-generation",
      "en",
      "license:apache-2.0"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://huggingface.co/nomic-ai/gpt4all-j",
    "requirements":{
      "required_classes":{
        "model": "AutoModelForCausalLM",
        "tokenizer": "AutoTokenizer"
      }
    },
    "model_desc": "GPT4All-J is an Apache-2 licensed chatbot trained over a massive curated corpus of assistant interactions, including word problems, multi-turn dialogue, code, poems, songs, and stories. It's a finetuned version of GPT-J optimized for assistant-style interactions.",
    "model_detail": "https://github.com/nomic-ai/gpt4all"
  },
  "meta-llama/Meta-Llama-3-8B-Instruct": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "llama",
      "meta",
      "llama-3",
      "text-generation",
      "text-generation-inference",
      "en",
      "license:llama3",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct",
    "model_desc": "Meta-Llama-3-8B-Instruct is part of the Meta Llama 3 family of large language models, optimized for instruction-following tasks. It's an 8B parameter model fine-tuned on a high-quality instruction dataset, making it particularly suitable for conversational AI and task-oriented applications.",
    "model_detail": "https://llama.meta.com/get-started/",
    "requirements":{
      "required_classes":{
        "model": "AutoModelForCausalLM",
        "tokenizer": "AutoTokenizer"
      },
      "requires_auth": true
    }
  },
  "jinaai/jina-reranker-v2-base-multilingual": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
        "Text Classification",
        "Transformers",
        "PyTorch",
        "ONNX",
        "Transformers.js",
        "multilingual",
        "reranker",
        "cross-encoder",
        "custom_code",
        "cc-by-nc-4.0",
        "EU Region: EU"
    ],
    "pipeline_tag": "text-classification",
    "model_card_url": "https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual",
    "requirements":{
      "required_classes":{
        "model": "AutoModelForSequenceClassification",
        "tokenizer": "AutoTokenizer"
      },
      "trust_remote_code": true,
      "required_packages": ["einops"]
    },
    "model_desc": ""
  },
  "lxyuan/distilbert-base-multilingual-cased-sentiments-student": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
        "Text Classification",
        "Transformers",
        "PyTorch"
    ],
    "pipeline_tag": "text-classification",
    "model_card_url": "https://huggingface.co/lxyuan/distilbert-base-multilingual-cased-sentiments-student",
    "requirements":{
      "required_classes":{
        "model": "AutoModelForSequenceClassification",
        "tokenizer": "AutoTokenizer"
      }
    },
    "model_desc": ""
  },
  "codellama/codellama-34b-instruct-hf": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "code-generation"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "Code Llama is an AI model built on top of Llama 2, fine-tuned for generating and discussing code.",
    "model_detail": "Code Llama is a pretrained and fine-tuned generative text models with 34 billion parameters. This model is designed for general code synthesis and understanding."
  },
  "google/flan-t5-xl": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "question-answering",
      "summarization",
      "retrieval-augmented-generation",
      "classification",
      "generation",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "A pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format.",
    "model_detail": "flan-t5-xl (3B) is a 3 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance."
  },
  "google/flan-t5-xxl": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "question-answering",
      "summarization",
      "retrieval-augmented-generation",
      "classification",
      "generation",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "flan-t5-xxl is an 11 billion parameter model based on the Flan-T5 family.",
    "model_detail": "flan-t5-xxl (11B) is an 11 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance."
  },
  "google/flan-ul2": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "question-answering",
      "summarization",
      "retrieval-augmented-generation",
      "classification",
      "generation",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "flan-ul2 is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net.",
    "model_detail": "flan-ul2 (20B) is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net (FLAN). Compared to the original UL2 model, flan-ul2 (20B) is more usable for few-shot in-context learning because it was trained with a three times larger receptive field. flan-ul2 (20B) outperforms flan-t5 (11B) by an overall relative improvement of +3.2%."
  },
  "ibm-mistralai/merlinite-7b": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "summarization",
      "retrieval-augmented-generation",
      "classification",
      "generation",
      "code",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "Merlinite-7b is a Mistral-7b-derivative model trained with the LAB methodology, using Mixtral-8x7b-Instruct as a teacher model.",
    "model_detail": "This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced."
  },
  "ibm/granite-13b-chat-v2": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "question-answering",
      "summarization",
      "classification",
      "generation",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.",
    "model_detail": "Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community."
  },
  "ibm/granite-13b-instruct-v2": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "question-answering",
      "summarization",
      "classification",
      "generation",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.",
    "model_detail": "Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community."
  },
  "ibm/granite-20b-multilingual": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "question-answering",
      "summarization",
      "retrieval-augmented-generation",
      "classification",
      "generation",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.",
    "model_detail": "Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community."
  },
  "ibm/granite-7b-lab": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "question-answering",
      "summarization",
      "retrieval-augmented-generation",
      "classification",
      "generation",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.",
    "model_detail": "Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community."
  },
  "meta-llama/llama-2-13b-chat": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "question-answering",
      "summarization",
      "retrieval-augmented-generation",
      "classification",
      "generation",
      "code",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "Llama-2-13b-chat is an auto-regressive language model that uses an optimized transformer architecture.",
    "model_detail": "Llama-2-13b-chat is a pretrained and fine-tuned generative text model with 13 billion parameters, optimized for dialogue use cases."
    },
  "meta-llama/llama-3-70b-instruct": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "question-answering",
      "summarization",
      "retrieval-augmented-generation",
      "classification",
      "generation",
      "code",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "Llama-3-70b-instruct is an auto-regressive language model that uses an optimized transformer architecture.",
    "model_detail": "Llama-3-70b-instruct is a pretrained and fine-tuned generative text model with 70 billion parameters, optimized for dialogue use cases."
    },
  "meta-llama/llama-3-8b-instruct": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "question-answering",
      "summarization",
      "retrieval-augmented-generation",
      "classification",
      "generation",
      "code",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "Llama-3-8b-instruct is an auto-regressive language model that uses an optimized transformer architecture.",
    "model_detail": "Llama-3-8b-instruct is a pretrained and fine-tuned generative text model with 8 billion parameters, optimized for dialogue use cases."
    },
  "mistralai/mixtral-8x7b-instruct-v01": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "summarization",
      "retrieval-augmented-generation",
      "classification",
      "generation",
      "code",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.",
    "model_detail": "This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below."
    },
  "ibm/natural-language-understanding": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonService",
    "tags": [
      "text-analysis",
      "sentiment-analysis",
      "emotion-detection",
      "entity-extraction",
      "keyword-extraction",
      "category-classification",
      "concept-tagging",
      "relation-extraction",
      "semantic-role-labeling"
    ],
    "pipeline_tag": "text-analysis",
    "model_card_url": "https://cloud.ibm.com/docs/natural-language-understanding",
    "model_desc": "IBM Watson Natural Language Understanding is a cloud-native product that uses deep learning to extract metadata from text such as entities, keywords, categories, sentiment, emotion, relations, and syntax.",
    "model_detail": "This service can analyze text to extract metadata from content such as concepts, entities, keywords, categories, sentiment, emotion, relations, and semantic roles using natural language processing."
  },
  "ibm/text-to-speech": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonService",
    "tags": [
      "text-to-speech",
      "audio-synthesis",
      "voice-generation",
      "speech-synthesis"
    ],
    "pipeline_tag": "text-to-speech",
    "model_card_url": "https://cloud.ibm.com/docs/text-to-speech",
    "model_desc": "IBM Watson Text to Speech service provides APIs that use IBM's speech-synthesis capabilities to convert written text to natural-sounding speech.",
    "model_detail": "This service supports multiple languages and voices, and includes neural voice technology for expressive and natural-sounding speech synthesis."
  },
  "ibm/speech-to-text": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonService",
    "tags": [
      "speech-to-text",
      "audio-transcription",
      "voice-recognition",
      "speech-recognition"
    ],
    "pipeline_tag": "speech-to-text",
    "model_card_url": "https://cloud.ibm.com/docs/speech-to-text",
    "model_desc": "IBM Watson Speech to Text service provides APIs that use IBM's speech-recognition capabilities to convert speech to text.",
    "model_detail": "This service can transcribe audio to text from various sources and formats. It uses machine intelligence to combine information about grammar and language structure with knowledge of the composition of the audio signal to generate an accurate transcription."
  },
  "ibm/slate-30m-english-rtrvr": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "embedding",
      "text-embedding",
      "semantic-search",
      "document-comparison",
      "retrieval-augmented-generation"
    ],
    "pipeline_tag": "feature-extraction",
    "model_card_url": "https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models-embed.html?context=wx",
    "model_desc": "The slate-30m-english-rtrvr is a distilled version of the slate-125m-english-rtrvr, provided by IBM. It's trained to maximize cosine similarity between two text inputs for later similarity evaluation.",
    "model_detail": "This embedding model has 6 layers, is faster than slate-125m-english-rtrvr, and is fine-tuned for sentence retrieval-based tasks. It generates 384-dimensional embeddings with a 512 token input limit.",
    "embedding_dimensions": 384,
    "max_input_tokens": 512,
    "supported_languages": ["English"]
  },
  "ibm/slate-125m-english-rtrvr": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "embedding",
      "text-embedding",
      "semantic-search",
      "document-comparison",
      "retrieval-augmented-generation"
    ],
    "pipeline_tag": "feature-extraction",
    "model_card_url": "https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models-embed.html?context=wx",
    "model_desc": "The slate-125m-english-rtrvr is an IBM-provided embedding model designed for text embedding tasks.",
    "model_detail": "This embedding model generates 768-dimensional embeddings with a 512 token input limit. It's the larger version compared to slate-30m-english-rtrvr.",
    "embedding_dimensions": 768,
    "max_input_tokens": 512,
    "supported_languages": ["English"]
  },
  "sentence-transformers/all-minilm-l12-v2": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "embedding",
      "text-embedding",
      "semantic-search",
      "sentence-similarity",
      "information-retrieval",
      "clustering"
    ],
    "pipeline_tag": "feature-extraction",
    "model_card_url": "https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2",
    "model_desc": "The all-minilm-l12-v2 is a sentence and short paragraph encoder provided by the open-source NLP community via Hugging Face.",
    "model_detail": "This model outputs 384-dimensional vectors capturing semantic information in text. It's fine-tuned on over 1 billion sentence pairs and is suitable for tasks like information retrieval, clustering, and sentence similarity detection.",
    "embedding_dimensions": 384,
    "max_input_tokens": 256,
    "supported_languages": ["English"]
  },
  "intfloat/multilingual-e5-large": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "embedding",
      "text-embedding",
      "multilingual",
      "semantic-search",
      "document-comparison",
      "retrieval-augmented-generation"
    ],
    "pipeline_tag": "feature-extraction",
    "model_card_url": "https://huggingface.co/intfloat/multilingual-e5-large",
    "model_desc": "The multilingual-e5-large is a multilingual embedding model built by Microsoft and provided via Hugging Face.",
    "model_detail": "This model has 24 layers and generates 1024-dimensional embeddings with a 512 token input limit. It supports up to 100 languages and is suitable for various multilingual embedding tasks.",
    "embedding_dimensions": 1024,
    "max_input_tokens": 512,
    "supported_languages": ["Multilingual (up to 100 languages)"]
  }
}