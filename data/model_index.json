{
  "openai-community/gpt2": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "tflite",
      "rust",
      "onnx",
      "safetensors",
      "gpt2",
      "text-generation",
      "exbert",
      "en",
      "doi:10.57967/hf/0039",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "text-generation-inference",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://huggingface.co/openai-community/gpt2",
    "requirements": {
      "required_classes": {
        "model": "AutoModelForCausalLM",
        "tokenizer": "AutoTokenizer"
      }
    },
    "config": {
      "model_config": {
        "use_cache": true
      },
      "tokenizer_config": {
        "do_lower_case": false,
        "eos_token": "<|endoftext|>",
        "unk_token": "<|endoftext|>"
      },
      "processor_config": {},
      "pipeline_config": {
        "max_length": 50,
        "num_return_sequences": 1,
        "top_k": 50,
        "top_p": 0.95,
        "temperature": 1.0,
        "repetition_penalty": 1.0,
        "no_repeat_ngram_size": 0
      },
      "device_config": {
          "device": "cpu"
      }
    }
  },
  "yolov8n": {
    "is_online": false,
    "model_source": "ultralytics",
    "model_class": "UltralyticsModel",
    "tags": [
      "yolo",
      "object-detection",
      "pytorch",
      "yolov8",
      "yolov8n",
      "computer-vision",
      "en",
      "license:agpl-3.0",
      "region:us"
    ],
    "model_desc": "Ultralytics YOLOv8 is a cutting-edge, state-of-the-art (SOTA) model designed for fast, accurate, and easy-to-use object detection. It builds upon the success of previous YOLO versions and introduces new features and improvements to further enhance performance and flexibility. YOLOv8 is suitable for a wide range of tasks including object detection, instance segmentation, and image classification. YOLOv8n is the smallest version of YOLOv8 series with 3.2M params.",
    "model_detail": "https://docs.ultralytics.com/models/yolov8/",
    "dataset_format": "yolo",
    "requirements": {
      "required_classes": {
        "model": "YOLO"
      }
    }
  },
  "yolov8s": {
    "is_online": false,
    "model_source": "ultralytics",
    "model_class": "UltralyticsModel",
    "tags": [
      "yolo",
      "object-detection",
      "pytorch",
      "yolov8",
      "yolov8s",
      "computer-vision",
      "en",
      "license:agpl-3.0"
    ],
    "model_desc": "Ultralytics YOLOv8 is a cutting-edge, state-of-the-art (SOTA) model designed for fast, accurate, and easy-to-use object detection. It builds upon the success of previous YOLO versions and introduces new features and improvements to further enhance performance and flexibility. YOLOv8 is suitable for a wide range of tasks including object detection, instance segmentation, and image classification. YOLOv8s has 11.2M params.",
    "model_detail": "https://docs.ultralytics.com/models/yolov8/",
    "dataset_format": "yolo",
    "requirements": {
      "required_classes": {
        "model": "YOLO"
      }
    }
  },
  "yolov10n": {
    "is_online": false,
    "model_source": "ultralytics",
    "model_class": "UltralyticsModel",
    "tags": [
      "yolo",
      "object-detection",
      "pytorch",
      "yolov10",
      "yolov10n",
      "computer-vision",
      "en",
      "license:agpl-3.0"
    ],
    "model_detail": "https://docs.ultralytics.com/models/yolov10/",
    "dataset_format": "yolo",
    "requirements": {
      "required_classes": {
        "model": "YOLO"
      }
    }
  },
  "yolov10s": {
    "is_online": false,
    "model_source": "ultralytics",
    "model_class": "UltralyticsModel",
    "tags": [
      "yolo",
      "object-detection",
      "pytorch",
      "yolov10",
      "yolov10s",
      "computer-vision",
      "en",
      "license:agpl-3.0"
    ],
    "model_detail": "https://docs.ultralytics.com/models/yolov10/",
    "dataset_format": "yolo",
    "requirements": {
      "required_classes": {
        "model": "YOLO"
      }
    }
  },
  "facebook/detr-resnet-50-panoptic": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "transformers",
      "pytorch",
      "vision",
      "object-detection",
      "instance-segmentation",
      "panoptic-segmentation",
      "detr",
      "en",
      "license:apache-2.0"
    ],
    "pipeline_tag": "image-segmentation",
    "model_card_url": "https://huggingface.co/facebook/detr-resnet-50-panoptic",
    "requirements": {
      "required_classes": {
        "model": "AutoModelForImageSegmentation",
        "image_processor": "AutoImageProcessor"
      },
      "required_packages": ["timm", "pillow"]
    },
    "config": {},
    "model_desc": "DETR (DEtection TRansformer) is an object detection model that uses a transformer-based architecture. This specific model is trained for panoptic segmentation, which combines both instance segmentation and semantic segmentation.",
    "model_detail": "https://github.com/facebookresearch/detr"
  },
  "google/owlvit-base-patch32": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "transformers",
      "pytorch",
      "owlvit",
      "object-detection",
      "vision",
      "en",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "object-detection",
    "model_card_url": "https://huggingface.co/google/owlvit-base-patch32",
    "requirements": {
      "required_classes": {
        "model": "OwlViTForObjectDetection",
        "processor": "OwlViTProcessor"
      }
    },
    "config": {
      "model_config": {},
      "tokenizer_config": {},
      "processor_config": {},
      "pipeline_config": {}
    }
  },
  "meta-llama/Meta-Llama-3-8B": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "llama",
      "meta",
      "llama-3",
      "text-generation",
      "text-generation-inference",
      "en",
      "license:llama3",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://huggingface.co/meta-llama/Meta-Llama-3-8B",
    "requirements": {
      "required_classes": {
        "model": "AutoModelForCausalLM",
        "tokenizer": "AutoTokenizer"
      },
      "requires_auth": true
    },
    "model_desc": "Meta-Llama-3-8B is part of the Meta Llama 3 family of large language models, optimized for dialogue use cases. It's an 8B parameter model that outperforms many available open-source chat models on common industry benchmarks.",
    "model_detail": "https://llama.meta.com/get-started/",
    "config": {
      "model_config": {},
      "tokenizer_config": {},
      "processor_config": {},
      "pipeline_config": {}
    }
  },
  "nomic-ai/gpt4all-j": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "gptj",
      "text-generation",
      "en",
      "license:apache-2.0"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://huggingface.co/nomic-ai/gpt4all-j",
    "requirements": {
      "required_classes": {
        "model": "AutoModelForCausalLM",
        "tokenizer": "AutoTokenizer"
      }
    },
    "model_desc": "GPT4All-J is an Apache-2 licensed chatbot trained over a massive curated corpus of assistant interactions, including word problems, multi-turn dialogue, code, poems, songs, and stories. It's a finetuned version of GPT-J optimized for assistant-style interactions.",
    "model_detail": "https://github.com/nomic-ai/gpt4all"
  },
  "meta-llama/Meta-Llama-3-8B-Instruct": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "llama",
      "meta",
      "llama-3",
      "text-generation",
      "text-generation-inference",
      "en",
      "license:llama3",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct",
    "model_desc": "Meta-Llama-3-8B-Instruct is part of the Meta Llama 3 family of large language models, optimized for instruction-following tasks. It's an 8B parameter model fine-tuned on a high-quality instruction dataset, making it particularly suitable for conversational AI and task-oriented applications.",
    "model_detail": "https://llama.meta.com/get-started/",
    "requirements": {
      "required_classes": {
        "model": "AutoModelForCausalLM",
        "tokenizer": "AutoTokenizer"
      },
      "requires_auth": true
    },
    "config": {
      "model_config": {
        "torch_dtype": "float16"
      },
      "tokenizer_config": {},
      "processor_config": {},
      "pipeline_config": {
        "max_length": 512,
        "max_new_tokens": 10000,
        "num_beams": 2,
        "use_cache": true
      },
      "device_config": {
        "device": "cuda"
        
      }
    }
  },
  "codellama/codellama-34b-instruct-hf": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "code-generation"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "Code Llama is an AI model built on top of Llama 2, fine-tuned for generating and discussing code.",
    "model_detail": "Code Llama is a pretrained and fine-tuned generative text models with 34 billion parameters. This model is designed for general code synthesis and understanding."
  },
  "google/flan-t5-xl": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "question-answering",
      "summarization",
      "retrieval-augmented-generation",
      "classification",
      "generation",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "A pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format.",
    "model_detail": "flan-t5-xl (3B) is a 3 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance."
  },
  "google/flan-t5-xxl": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "question-answering",
      "summarization",
      "retrieval-augmented-generation",
      "classification",
      "generation",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "flan-t5-xxl is an 11 billion parameter model based on the Flan-T5 family.",
    "model_detail": "flan-t5-xxl (11B) is an 11 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance."
  },
  "google/flan-ul2": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "question-answering",
      "summarization",
      "retrieval-augmented-generation",
      "classification",
      "generation",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "flan-ul2 is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net.",
    "model_detail": "flan-ul2 (20B) is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net (FLAN). Compared to the original UL2 model, flan-ul2 (20B) is more usable for few-shot in-context learning because it was trained with a three times larger receptive field. flan-ul2 (20B) outperforms flan-t5 (11B) by an overall relative improvement of +3.2%."
  },
  "ibm-mistralai/merlinite-7b": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "summarization",
      "retrieval-augmented-generation",
      "classification",
      "generation",
      "code",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "Merlinite-7b is a Mistral-7b-derivative model trained with the LAB methodology, using Mixtral-8x7b-Instruct as a teacher model.",
    "model_detail": "This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced."
  },
  "ibm/granite-13b-chat-v2": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "question-answering",
      "summarization",
      "classification",
      "generation",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.",
    "model_detail": "Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community."
  },
  "ibm/granite-13b-instruct-v2": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "question-answering",
      "summarization",
      "classification",
      "generation",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.",
    "model_detail": "Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community."
  },
  "ibm/granite-20b-multilingual": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "question-answering",
      "summarization",
      "retrieval-augmented-generation",
      "classification",
      "generation",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.",
    "model_detail": "Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community."
  },
  "ibm/granite-7b-lab": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "question-answering",
      "summarization",
      "retrieval-augmented-generation",
      "classification",
      "generation",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.",
    "model_detail": "Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community."
  },
  "meta-llama/llama-2-13b-chat": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "question-answering",
      "summarization",
      "retrieval-augmented-generation",
      "classification",
      "generation",
      "code",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "Llama-2-13b-chat is an auto-regressive language model that uses an optimized transformer architecture.",
    "model_detail": "Llama-2-13b-chat is a pretrained and fine-tuned generative text model with 13 billion parameters, optimized for dialogue use cases."
    },
  "meta-llama/llama-3-70b-instruct": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "question-answering",
      "summarization",
      "retrieval-augmented-generation",
      "classification",
      "generation",
      "code",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "Llama-3-70b-instruct is an auto-regressive language model that uses an optimized transformer architecture.",
    "model_detail": "Llama-3-70b-instruct is a pretrained and fine-tuned generative text model with 70 billion parameters, optimized for dialogue use cases."
    },
  "meta-llama/llama-3-8b-instruct": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "question-answering",
      "summarization",
      "retrieval-augmented-generation",
      "classification",
      "generation",
      "code",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "Llama-3-8b-instruct is an auto-regressive language model that uses an optimized transformer architecture.",
    "model_detail": "Llama-3-8b-instruct is a pretrained and fine-tuned generative text model with 8 billion parameters, optimized for dialogue use cases."
    },
  "mistralai/mixtral-8x7b-instruct-v01": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonModel",
    "tags": [
      "text-generation",
      "summarization",
      "retrieval-augmented-generation",
      "classification",
      "generation",
      "code",
      "extraction"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://www.ibm.com/products/watsonx-ai/foundation-models",
    "model_desc": "The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.",
    "model_detail": "This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below."
    },
  "ibm/natural-language-understanding": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonService",
    "tags": [
      "text-analysis",
      "sentiment-analysis",
      "emotion-detection",
      "entity-extraction",
      "keyword-extraction",
      "category-classification",
      "concept-tagging",
      "relation-extraction",
      "semantic-role-labeling"
    ],
    "pipeline_tag": "text-analysis",
    "model_card_url": "https://cloud.ibm.com/docs/natural-language-understanding",
    "model_desc": "IBM Watson Natural Language Understanding is a cloud-native product that uses deep learning to extract metadata from text such as entities, keywords, categories, sentiment, emotion, relations, and syntax.",
    "model_detail": "This service can analyze text to extract metadata from content such as concepts, entities, keywords, categories, sentiment, emotion, relations, and semantic roles using natural language processing."
  },
  "ibm/text-to-speech": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonService",
    "tags": [
      "text-to-speech",
      "audio-synthesis",
      "voice-generation",
      "speech-synthesis"
    ],
    "pipeline_tag": "text-to-speech",
    "model_card_url": "https://cloud.ibm.com/docs/text-to-speech",
    "model_desc": "IBM Watson Text to Speech service provides APIs that use IBM's speech-synthesis capabilities to convert written text to natural-sounding speech.",
    "model_detail": "This service supports multiple languages and voices, and includes neural voice technology for expressive and natural-sounding speech synthesis."
  },
  "ibm/speech-to-text": {
    "is_online": true,
    "model_source": "IBM",
    "model_class": "WatsonService",
    "tags": [
      "speech-to-text",
      "audio-transcription",
      "voice-recognition",
      "speech-recognition"
    ],
    "pipeline_tag": "speech-to-text",
    "model_card_url": "https://cloud.ibm.com/docs/speech-to-text",
    "model_desc": "IBM Watson Speech to Text service provides APIs that use IBM's speech-recognition capabilities to convert speech to text.",
    "model_detail": "This service can transcribe audio to text from various sources and formats. It uses machine intelligence to combine information about grammar and language structure with knowledge of the composition of the audio signal to generate an accurate transcription."
  },
  "cardiffnlp/twitter-roberta-base-sentiment-latest": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "Text Classification",
      "Sentiment Analysis"
    ],
    "languages": [
      "English"
    ],
    "labels" : [
      "positive",
      "neutral",
      "negative"
    ],
    "pipeline_tag": "text-classification",
    "model_desc": "This is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark. This model is suitable for English. ",
    "model_card_url": "https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest",
    "requirements": {
      "required_classes": {
        "model": "AutoModelForSequenceClassification",
        "tokenizer": "AutoTokenizer"
      }
    }
  },
  "distilbert/distilbert-base-uncased-finetuned-sst-2-english": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "Text Classification",
      "Sentiment Analysis"
    ],
    "languages": [
      "English"
    ],
    "labels" : [
      "negative",
      "positive"
    ],
    "pipeline_tag": "text-classification",
    "model_desc": "This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. This model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7).",
    "model_card_url": "https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english",
    "requirements": {
      "required_classes": {
        "model": "AutoModelForSequenceClassification",
        "tokenizer": "AutoTokenizer"
      }
    }
  },
  "nlptown/bert-base-multilingual-uncased-sentiment": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "Text Classification",
      "Product Reviews"
    ],
    "languages": [
      "English",
      "Dutch",
      "German",
      "French",
      "Italian",
      "Spanish"
    ],
    "labels" : [
      "5 stars",
      "4 stars",
      "3 stars",
      "2 stars",
      "1 star"
    ],
    "pipeline_tag": "text-classification",
    "model_desc": "This is a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish, and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5). This model is intended for direct use as a sentiment analysis model for product reviews in any of the six languages above or for further finetuning on related sentiment analysis tasks.",
    "model_card_url": "https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment",
    "requirements": {
      "required_classes": {
        "model": "AutoModelForSequenceClassification",
        "tokenizer": "AutoTokenizer"
      }
    }
  },
  "jinaai/jina-reranker-v2-base-multilingual": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
        "Text Classification",
        "Text Reranker",
        "Multilingual"
    ],
    "languages": [
      "Arabic",
      "Bengali",
      "Bulgarian",
      "Chinese (Simplified)",
      "Chinese (Traditional)",
      "Croatian",
      "Czech",
      "Danish",
      "Dutch",
      "English",
      "Estonian",
      "Finnish",
      "French",
      "German",
      "Greek",
      "Gujarati",
      "Hebrew",
      "Hindi",
      "Hungarian",
      "Indonesian",
      "Italian",
      "Japanese",
      "Kannada",
      "Korean",
      "Latvian",
      "Lithuanian",
      "Malayalam",
      "Marathi",
      "Norwegian",
      "Polish",
      "Portuguese",
      "Punjabi",
      "Romanian",
      "Russian",
      "Slovak",
      "Slovenian",
      "Spanish",
      "Swedish",
      "Tamil",
      "Telugu",
      "Thai",
      "Turkish",
      "Ukrainian",
      "Urdu",
      "Vietnamese"
    ],
    "pipeline_tag": "text-classification",
    "model_card_url": "https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual",
    "requirements": {
      "required_classes": {
        "model": "AutoModelForSequenceClassification",
        "tokenizer": "AutoTokenizer"
      },
      "required_packages": ["einops"]
    },
    "config": {
      "model_config": {
        "trust_remote_code": true
      },
      "tokenizer_config": {
          "trust_remote_code": true
      },
      "pipeline_config": {
        "trust_remote_code": true
      }
    },
    "model_desc": "The Jina Reranker v2 (jina-reranker-v2-base-multilingual) is a transformer-based model that has been fine-tuned for text reranking task, which is a crucial component in many information retrieval systems. It is a cross-encoder model that takes a query and a document pair as input and outputs a score indicating the relevance of the document to the query. The model is trained on a large dataset of query-document pairs and is capable of reranking documents in multiple languages with high accuracy. The jina-reranker-v2-base-multilingual model is capable of handling long texts with a context length of up to 1024 tokens, enabling the processing of extensive inputs. To enable the model to handle long texts that exceed 1024 tokens, the model uses a sliding window approach to chunk the input text into smaller pieces and rerank each chunk separately."
  },
  "google-t5/t5-large": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "Translation",
      "Multilingual"
    ],
    "languages": {
      "en": "English",
      "fr": "French",
      "ro": "Romanian",
      "de": "German"
    },
    "pipeline_tag": "translation",
    "model_desc": "With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.",
    "model_card_url": "https://huggingface.co/google-t5/t5-large",
    "requirements": {
      "required_classes": {
        "model": "AutoModelForSeq2SeqLM",
        "tokenizer": "AutoTokenizer"
      }
    },
    "config": {
      "translation_config": {
        "src_lang": "en",
        "tgt_lang": "fr"
      }
    }
  },
  "google/madlad400-10b-mt": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "Translation",
      "Multilingual"
    ],
    "languages": {
      "en": "English",
      "ru": "Russian",
      "es": "Spanish",
      "fr": "French",
      "de": "German",
      "it": "Italian",
      "pt": "Portuguese",
      "pl": "Polish",
      "nl": "Dutch",
      "vi": "Vietnamese",
      "tr": "Turkish",
      "sv": "Swedish",
      "id": "Indonesian",
      "ro": "Romanian",
      "cs": "Czech",
      "zh": "Chinese",
      "hu": "Hungarian",
      "ja": "Japanese",
      "th": "Thai",
      "fi": "Finnish",
      "fa": "Persian",
      "uk": "Ukrainian",
      "da": "Danish",
      "el": "Greek",
      "no": "Norwegian",
      "bg": "Bulgarian",
      "sk": "Slovak",
      "ko": "Korean",
      "ar": "Arabic",
      "lt": "Lithuanian",
      "ca": "Catalan",
      "sl": "Slovenian",
      "he": "Hebrew",
      "et": "Estonian",
      "lv": "Latvian",
      "hi": "Hindi",
      "sq": "Albanian",
      "ms": "Malay",
      "az": "Azerbaijani",
      "sr": "Serbian",
      "ta": "Tamil",
      "hr": "Croatian",
      "kk": "Kazakh",
      "is": "Icelandic",
      "ml": "Malayalam",
      "mr": "Marathi",
      "te": "Telugu",
      "af": "Afrikaans",
      "gl": "Galician",
      "fil": "Filipino",
      "be": "Belarusian",
      "mk": "Macedonian",
      "eu": "Basque",
      "bn": "Bengali",
      "ka": "Georgian",
      "mn": "Mongolian",
      "bs": "Bosnian",
      "uz": "Uzbek",
      "ur": "Urdu",
      "sw": "Swahili",
      "yue": "Cantonese",
      "ne": "Nepali",
      "kn": "Kannada",
      "kaa": "Kara-Kalpak",
      "gu": "Gujarati",
      "si": "Sinhala",
      "cy": "Welsh",
      "eo": "Esperanto",
      "la": "Latin",
      "hy": "Armenian",
      "ky": "Kyrgyz",
      "tg": "Tajik",
      "ga": "Irish",
      "mt": "Maltese",
      "my": "Burmese",
      "km": "Khmer",
      "tt": "Tatar",
      "so": "Somali",
      "ku": "Kurdish",
      "ps": "Pashto",
      "pa": "Punjabi",
      "rw": "Kinyarwanda",
      "lo": "Lao",
      "ha": "Hausa",
      "dv": "Dhivehi",
      "fy": "Frisian",
      "lb": "Luxembourgish",
      "ckb": "Central Kurdish",
      "mg": "Malagasy",
      "gd": "Scottish Gaelic",
      "am": "Amharic",
      "ug": "Uyghur",
      "ht": "Haitian Creole",
      "grc": "Ancient Greek",
      "hmn": "Hmong",
      "sd": "Sindhi",
      "jv": "Javanese",
      "mi": "Maori",
      "tk": "Turkmen",
      "ceb": "Cebuano",
      "yi": "Yiddish",
      "ba": "Bashkir",
      "fo": "Faroese",
      "or": "Odia",
      "xh": "Xhosa",
      "su": "Sundanese",
      "kl": "Kalaallisut",
      "ny": "Chichewa",
      "sm": "Samoan",
      "sn": "Shona",
      "co": "Corsican",
      "zu": "Zulu",
      "ig": "Igbo",
      "yo": "Yoruba",
      "pap": "Papiamento",
      "st": "Sesotho",
      "haw": "Hawaiian",
      "as": "Assamese",
      "oc": "Occitan",
      "cv": "Chuvash",
      "lus": "Mizo",
      "tet": "Tetum",
      "gsw": "Swiss German",
      "sah": "Yakut",
      "br": "Breton",
      "rm": "Romansh",
      "sa": "Sanskrit",
      "bo": "Tibetan",
      "om": "Oromo",
      "se": "Northern Sami",
      "ce": "Chechen",
      "cnh": "Hakha Chin",
      "ilo": "Ilocano",
      "hil": "Hiligaynon",
      "udm": "Udmurt",
      "os": "Ossetic",
      "lg": "Ganda",
      "ti": "Tigrinya",
      "vec": "Venetian",
      "ts": "Tsonga",
      "tyv": "Tuvinian",
      "kbd": "Kabardian",
      "ee": "Ewe",
      "iba": "Iban",
      "av": "Avaric",
      "kha": "Khasi",
      "to": "Tongan",
      "tn": "Tswana",
      "nso": "Northern Sotho",
      "fj": "Fijian",
      "zza": "Zaza",
      "ak": "Akan",
      "ada": "Adangme",
      "otq": "Querétaro Otomi",
      "dz": "Dzongkha",
      "bua": "Buriat",
      "cfm": "Falam Chin",
      "ln": "Lingala",
      "chm": "Mari",
      "gn": "Guarani",
      "krc": "Karachay-Balkar",
      "wa": "Walloon",
      "hif": "Fiji Hindi",
      "yua": "Yucatec Maya",
      "srn": "Sranan Tongo",
      "war": "Waray",
      "rom": "Romany",
      "bik": "Bikol",
      "pam": "Pampanga",
      "sg": "Sango",
      "lu": "Luba-Katanga",
      "ady": "Adyghe",
      "kbp": "Kabiyè",
      "syr": "Syriac",
      "ltg": "Latgalian",
      "myv": "Erzya",
      "iso": "Isoko",
      "kac": "Kachin",
      "bho": "Bhojpuri",
      "ay": "Aymara",
      "kum": "Kumyk",
      "qu": "Quechua",
      "za": "Zhuang",
      "pag": "Pangasinan",
      "ngu": "Guerrero Nahuatl",
      "ve": "Venda",
      "pck": "Paite Chin",
      "zap": "Zapotec",
      "tyz": "Tày",
      "hui": "Huilliche",
      "bbc": "Batak Toba",
      "tzo": "Tzotzil",
      "tiv": "Tiv",
      "ksd": "Tolai",
      "gom": "Goan Konkani",
      "min": "Minangkabau",
      "ang": "Old English",
      "nhe": "Eastern Huasteca Nahuatl",
      "bgp": "Eastern Balochi",
      "nzi": "Nzema",
      "nnb": "Nande",
      "nv": "Navajo",
      "zxx": "No linguistic content",
      "bci": "Baoulé",
      "kv": "Komi",
      "new": "Newari",
      "mps": "Masan",
      "alt": "Southern Altai",
      "meu": "Motu",
      "bew": "Betawi",
      "fon": "Fon",
      "iu": "Inuktitut",
      "abt": "Ambulas",
      "mgh": "Makhuwa-Meetto",
      "mnw": "Mon",
      "tvl": "Tuvalu",
      "dov": "Dombe",
      "tlh": "Klingon",
      "ho": "Hiri Motu",
      "kw": "Cornish",
      "mrj": "Western Mari",
      "meo": "Kedah Malay",
      "crh": "Crimean Tatar",
      "mbt": "Matigsalug Manobo",
      "emp": "Northern Emberá",
      "ace": "Achinese",
      "ium": "Iu Mien",
      "mam": "Mam",
      "gym": "Ngäbere",
      "mai": "Maithili",
      "crs": "Seselwa Creole French",
      "pon": "Pohnpeian",
      "ubu": "Umbu-Ungu",
      "fip": "Fipa",
      "quc": "K'iche'",
      "gv": "Manx",
      "kj": "Kuanyama",
      "btx": "Batak Karo",
      "ape": "Bukiyip",
      "chk": "Chuukese",
      "rcf": "Réunion Creole French",
      "shn": "Shan",
      "tzh": "Tzeltal",
      "mdf": "Moksha",
      "ppk": "Uma",
      "ss": "Swati",
      "gag": "Gagauz",
      "cab": "Garifuna",
      "kri": "Krio",
      "seh": "Sena",
      "ibb": "Ibibio",
      "tbz": "Ditammari",
      "bru": "Eastern Bru",
      "enq": "Enga",
      "ach": "Acoli",
      "cuk": "San Blas Kuna",
      "kmb": "Kimbundu",
      "wo": "Wolof",
      "kek": "Kekchí",
      "qub": "Huallaga Huánuco Quechua",
      "tab": "Tabassaran",
      "bts": "Batak Simalungun",
      "kos": "Kosraean",
      "rwo": "Rawa",
      "cak": "Kaqchikel",
      "tuc": "Mutu",
      "bum": "Bulu",
      "cjk": "Chokwe",
      "gil": "Gilbertese",
      "stq": "Saterland Frisian",
      "tsg": "Tausug",
      "quh": "South Bolivian Quechua",
      "mak": "Makasar",
      "arn": "Mapudungun",
      "ban": "Balinese",
      "jiv": "Shuar",
      "sja": "Epena",
      "yap": "Yapese",
      "tcy": "Tulu",
      "toj": "Tojolabal",
      "twu": "Termanu",
      "xal": "Kalmyk",
      "amu": "Guerrero Amuzgo",
      "rmc": "Carpathian Romani",
      "hus": "Huastec",
      "nia": "Nias",
      "kjh": "Khakas",
      "bm": "Bambara",
      "guh": "Guahibo",
      "mas": "Masai",
      "acf": "Saint Lucian Creole French",
      "dtp": "Central Dusun",
      "ksw": "S'gaw Karen",
      "bzj": "Belize Kriol English",
      "din": "Dinka",
      "zne": "Zande",
      "mad": "Madurese",
      "msi": "Sabah Malay",
      "mag": "Magahi",
      "mkn": "Kupang Malay",
      "kg": "Kongo",
      "lhu": "Lahu",
      "ch": "Chamorro",
      "qvi": "Imbabura Highland Quichua",
      "mh": "Marshallese",
      "djk": "Aukan",
      "sus": "Susu",
      "mfe": "Morisyen",
      "srm": "Saramaccan",
      "dyu": "Dyula",
      "ctu": "Chol",
      "gui": "Eastern Bolivian Guaraní",
      "pau": "Palauan",
      "inb": "Inga",
      "bi": "Bislama",
      "mni": "Manipuri",
      "guc": "Wayuu",
      "jam": "Jamaican Creole English",
      "wal": "Wolaytta",
      "jac": "Popti'",
      "bas": "Basaa",
      "gor": "Gorontalo",
      "skr": "Saraiki",
      "nyu": "Nyungwe",
      "noa": "Wounaan",
      "sda": "Toraja-Sa'dan",
      "gub": "Guajajara",
      "nog": "Nogai",
      "cni": "Asháninka",
      "teo": "Teso",
      "tdx": "Tandroy-Mahafaly Malagasy",
      "sxn": "Sangir",
      "rki": "Rakhine",
      "nr": "South Ndebele",
      "frp": "Arpitan",
      "alz": "Alur",
      "taj": "Eastern Tamang",
      "lrc": "Northern Luri",
      "cce": "Chopi",
      "rn": "Rundi",
      "jvn": "Caribbean Javanese",
      "hvn": "Sabu",
      "nij": "Ngaju",
      "dwr": "Dawro",
      "izz": "Izii",
      "msm": "Agusan Manobo",
      "bus": "Bokobaru",
      "ktu": "Kituba",
      "chr": "Cherokee",
      "maz": "Central Mazahua",
      "tzj": "Tz'utujil",
      "suz": "Sunwar",
      "knj": "Western Kanjobal",
      "bim": "Bimoba",
      "gvl": "Gulay",
      "bqc": "Boko",
      "tca": "Ticuna",
      "pis": "Pijin",
      "prk": "Parauk",
      "laj": "Lango",
      "mel": "Central Melanau",
      "qxr": "Cañar Highland Quichua",
      "niq": "Nandi",
      "ahk": "Akha",
      "shp": "Shipibo-Conibo",
      "hne": "Chhattisgarhi",
      "spp": "Supyire Senoufo",
      "koi": "Komi-Permyak",
      "krj": "Kinaray-a",
      "quf": "Lambayeque Quechua",
      "luz": "Southern Luri",
      "agr": "Aguaruna",
      "tsc": "Tswa",
      "mqy": "Manggarai",
      "gof": "Gofa",
      "gbm": "Garhwali",
      "miq": "Mískito",
      "dje": "Zarma",
      "awa": "Awadhi",
      "bjj": "Kanauji",
      "qvz": "Quichua",
      "sjp": "Surjapuri",
      "tll": "Tetela",
      "raj": "Rajasthani",
      "kjg": "Khmu",
      "bgz": "Banggai",
      "quy": "Ayacucho Quechua",
      "cbk": "Chavacano",
      "akb": "Batak Angkola",
      "oj": "Ojibwa",
      "ify": "Kallahan",
      "mey": "Hassaniyya",
      "ks": "Kashmiri",
      "cac": "Chuj",
      "brx": "Bodo",
      "qup": "Southern Pastaza Quechua",
      "syl": "Sylheti",
      "jax": "Jambi Malay",
      "ff": "Fulah",
      "ber": "Berber",
      "tks": "Takestani",
      "trp": "Kok Borok",
      "mrw": "Maranao",
      "adh": "Adhola",
      "smt": "Simte",
      "srr": "Serer",
      "ffm": "Maasina Fulfulde",
      "qvc": "Cajamarca Quechua",
      "mtr": "Mewari",
      "ann": "Obolo",
      "aa": "Afar",
      "noe": "Nimadi",
      "nut": "Nung",
      "gyn": "Guyanese Creole English",
      "kwi": "Awa-Cuaiquer",
      "xmm": "Manado Malay",
      "msb": "Masbatenyo"
      },
      "pipeline_tag": "translation",    
      "model_desc": "MADLAD-400-10B-MT is a multilingual machine translation model based on the T5 architecture that was trained on 250 billion tokens covering over 450 languages using publicly     available data. It is competitive with models that are significantly larger.",
      "model_card_url": "https://huggingface.co/google/madlad400-10b-mt",
      "requirements": {
        "required_classes": {
          "model": "AutoModelForSeq2SeqLM",
          "tokenizer": "AutoTokenizer"
        }
      }
  },
  "google/madlad400-3b-mt": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "Translation",
      "Multilingual"
    ],
    "languages": {
      "en": "English",
      "ru": "Russian",
      "es": "Spanish",
      "fr": "French",
      "de": "German",
      "it": "Italian",
      "pt": "Portuguese",
      "pl": "Polish",
      "nl": "Dutch",
      "vi": "Vietnamese",
      "tr": "Turkish",
      "sv": "Swedish",
      "id": "Indonesian",
      "ro": "Romanian",
      "cs": "Czech",
      "zh": "Chinese",
      "hu": "Hungarian",
      "ja": "Japanese",
      "th": "Thai",
      "fi": "Finnish",
      "fa": "Persian",
      "uk": "Ukrainian",
      "da": "Danish",
      "el": "Greek",
      "no": "Norwegian",
      "bg": "Bulgarian",
      "sk": "Slovak",
      "ko": "Korean",
      "ar": "Arabic",
      "lt": "Lithuanian",
      "ca": "Catalan",
      "sl": "Slovenian",
      "he": "Hebrew",
      "et": "Estonian",
      "lv": "Latvian",
      "hi": "Hindi",
      "sq": "Albanian",
      "ms": "Malay",
      "az": "Azerbaijani",
      "sr": "Serbian",
      "ta": "Tamil",
      "hr": "Croatian",
      "kk": "Kazakh",
      "is": "Icelandic",
      "ml": "Malayalam",
      "mr": "Marathi",
      "te": "Telugu",
      "af": "Afrikaans",
      "gl": "Galician",
      "fil": "Filipino",
      "be": "Belarusian",
      "mk": "Macedonian",
      "eu": "Basque",
      "bn": "Bengali",
      "ka": "Georgian",
      "mn": "Mongolian",
      "bs": "Bosnian",
      "uz": "Uzbek",
      "ur": "Urdu",
      "sw": "Swahili",
      "yue": "Cantonese",
      "ne": "Nepali",
      "kn": "Kannada",
      "kaa": "Kara-Kalpak",
      "gu": "Gujarati",
      "si": "Sinhala",
      "cy": "Welsh",
      "eo": "Esperanto",
      "la": "Latin",
      "hy": "Armenian",
      "ky": "Kyrgyz",
      "tg": "Tajik",
      "ga": "Irish",
      "mt": "Maltese",
      "my": "Burmese",
      "km": "Khmer",
      "tt": "Tatar",
      "so": "Somali",
      "ku": "Kurdish",
      "ps": "Pashto",
      "pa": "Punjabi",
      "rw": "Kinyarwanda",
      "lo": "Lao",
      "ha": "Hausa",
      "dv": "Dhivehi",
      "fy": "Frisian",
      "lb": "Luxembourgish",
      "ckb": "Central Kurdish",
      "mg": "Malagasy",
      "gd": "Scottish Gaelic",
      "am": "Amharic",
      "ug": "Uyghur",
      "ht": "Haitian Creole",
      "grc": "Ancient Greek",
      "hmn": "Hmong",
      "sd": "Sindhi",
      "jv": "Javanese",
      "mi": "Maori",
      "tk": "Turkmen",
      "ceb": "Cebuano",
      "yi": "Yiddish",
      "ba": "Bashkir",
      "fo": "Faroese",
      "or": "Odia",
      "xh": "Xhosa",
      "su": "Sundanese",
      "kl": "Kalaallisut",
      "ny": "Chichewa",
      "sm": "Samoan",
      "sn": "Shona",
      "co": "Corsican",
      "zu": "Zulu",
      "ig": "Igbo",
      "yo": "Yoruba",
      "pap": "Papiamento",
      "st": "Sesotho",
      "haw": "Hawaiian",
      "as": "Assamese",
      "oc": "Occitan",
      "cv": "Chuvash",
      "lus": "Mizo",
      "tet": "Tetum",
      "gsw": "Swiss German",
      "sah": "Yakut",
      "br": "Breton",
      "rm": "Romansh",
      "sa": "Sanskrit",
      "bo": "Tibetan",
      "om": "Oromo",
      "se": "Northern Sami",
      "ce": "Chechen",
      "cnh": "Hakha Chin",
      "ilo": "Ilocano",
      "hil": "Hiligaynon",
      "udm": "Udmurt",
      "os": "Ossetic",
      "lg": "Ganda",
      "ti": "Tigrinya",
      "vec": "Venetian",
      "ts": "Tsonga",
      "tyv": "Tuvinian",
      "kbd": "Kabardian",
      "ee": "Ewe",
      "iba": "Iban",
      "av": "Avaric",
      "kha": "Khasi",
      "to": "Tongan",
      "tn": "Tswana",
      "nso": "Northern Sotho",
      "fj": "Fijian",
      "zza": "Zaza",
      "ak": "Akan",
      "ada": "Adangme",
      "otq": "Querétaro Otomi",
      "dz": "Dzongkha",
      "bua": "Buriat",
      "cfm": "Falam Chin",
      "ln": "Lingala",
      "chm": "Mari",
      "gn": "Guarani",
      "krc": "Karachay-Balkar",
      "wa": "Walloon",
      "hif": "Fiji Hindi",
      "yua": "Yucatec Maya",
      "srn": "Sranan Tongo",
      "war": "Waray",
      "rom": "Romany",
      "bik": "Bikol",
      "pam": "Pampanga",
      "sg": "Sango",
      "lu": "Luba-Katanga",
      "ady": "Adyghe",
      "kbp": "Kabiyè",
      "syr": "Syriac",
      "ltg": "Latgalian",
      "myv": "Erzya",
      "iso": "Isoko",
      "kac": "Kachin",
      "bho": "Bhojpuri",
      "ay": "Aymara",
      "kum": "Kumyk",
      "qu": "Quechua",
      "za": "Zhuang",
      "pag": "Pangasinan",
      "ngu": "Guerrero Nahuatl",
      "ve": "Venda",
      "pck": "Paite Chin",
      "zap": "Zapotec",
      "tyz": "Tày",
      "hui": "Huilliche",
      "bbc": "Batak Toba",
      "tzo": "Tzotzil",
      "tiv": "Tiv",
      "ksd": "Tolai",
      "gom": "Goan Konkani",
      "min": "Minangkabau",
      "ang": "Old English",
      "nhe": "Eastern Huasteca Nahuatl",
      "bgp": "Eastern Balochi",
      "nzi": "Nzema",
      "nnb": "Nande",
      "nv": "Navajo",
      "zxx": "No linguistic content",
      "bci": "Baoulé",
      "kv": "Komi",
      "new": "Newari",
      "mps": "Masan",
      "alt": "Southern Altai",
      "meu": "Motu",
      "bew": "Betawi",
      "fon": "Fon",
      "iu": "Inuktitut",
      "abt": "Ambulas",
      "mgh": "Makhuwa-Meetto",
      "mnw": "Mon",
      "tvl": "Tuvalu",
      "dov": "Dombe",
      "tlh": "Klingon",
      "ho": "Hiri Motu",
      "kw": "Cornish",
      "mrj": "Western Mari",
      "meo": "Kedah Malay",
      "crh": "Crimean Tatar",
      "mbt": "Matigsalug Manobo",
      "emp": "Northern Emberá",
      "ace": "Achinese",
      "ium": "Iu Mien",
      "mam": "Mam",
      "gym": "Ngäbere",
      "mai": "Maithili",
      "crs": "Seselwa Creole French",
      "pon": "Pohnpeian",
      "ubu": "Umbu-Ungu",
      "fip": "Fipa",
      "quc": "K'iche'",
      "gv": "Manx",
      "kj": "Kuanyama",
      "btx": "Batak Karo",
      "ape": "Bukiyip",
      "chk": "Chuukese",
      "rcf": "Réunion Creole French",
      "shn": "Shan",
      "tzh": "Tzeltal",
      "mdf": "Moksha",
      "ppk": "Uma",
      "ss": "Swati",
      "gag": "Gagauz",
      "cab": "Garifuna",
      "kri": "Krio",
      "seh": "Sena",
      "ibb": "Ibibio",
      "tbz": "Ditammari",
      "bru": "Eastern Bru",
      "enq": "Enga",
      "ach": "Acoli",
      "cuk": "San Blas Kuna",
      "kmb": "Kimbundu",
      "wo": "Wolof",
      "kek": "Kekchí",
      "qub": "Huallaga Huánuco Quechua",
      "tab": "Tabassaran",
      "bts": "Batak Simalungun",
      "kos": "Kosraean",
      "rwo": "Rawa",
      "cak": "Kaqchikel",
      "tuc": "Mutu",
      "bum": "Bulu",
      "cjk": "Chokwe",
      "gil": "Gilbertese",
      "stq": "Saterland Frisian",
      "tsg": "Tausug",
      "quh": "South Bolivian Quechua",
      "mak": "Makasar",
      "arn": "Mapudungun",
      "ban": "Balinese",
      "jiv": "Shuar",
      "sja": "Epena",
      "yap": "Yapese",
      "tcy": "Tulu",
      "toj": "Tojolabal",
      "twu": "Termanu",
      "xal": "Kalmyk",
      "amu": "Guerrero Amuzgo",
      "rmc": "Carpathian Romani",
      "hus": "Huastec",
      "nia": "Nias",
      "kjh": "Khakas",
      "bm": "Bambara",
      "guh": "Guahibo",
      "mas": "Masai",
      "acf": "Saint Lucian Creole French",
      "dtp": "Central Dusun",
      "ksw": "S'gaw Karen",
      "bzj": "Belize Kriol English",
      "din": "Dinka",
      "zne": "Zande",
      "mad": "Madurese",
      "msi": "Sabah Malay",
      "mag": "Magahi",
      "mkn": "Kupang Malay",
      "kg": "Kongo",
      "lhu": "Lahu",
      "ch": "Chamorro",
      "qvi": "Imbabura Highland Quichua",
      "mh": "Marshallese",
      "djk": "Aukan",
      "sus": "Susu",
      "mfe": "Morisyen",
      "srm": "Saramaccan",
      "dyu": "Dyula",
      "ctu": "Chol",
      "gui": "Eastern Bolivian Guaraní",
      "pau": "Palauan",
      "inb": "Inga",
      "bi": "Bislama",
      "mni": "Manipuri",
      "guc": "Wayuu",
      "jam": "Jamaican Creole English",
      "wal": "Wolaytta",
      "jac": "Popti'",
      "bas": "Basaa",
      "gor": "Gorontalo",
      "skr": "Saraiki",
      "nyu": "Nyungwe",
      "noa": "Wounaan",
      "sda": "Toraja-Sa'dan",
      "gub": "Guajajara",
      "nog": "Nogai",
      "cni": "Asháninka",
      "teo": "Teso",
      "tdx": "Tandroy-Mahafaly Malagasy",
      "sxn": "Sangir",
      "rki": "Rakhine",
      "nr": "South Ndebele",
      "frp": "Arpitan",
      "alz": "Alur",
      "taj": "Eastern Tamang",
      "lrc": "Northern Luri",
      "cce": "Chopi",
      "rn": "Rundi",
      "jvn": "Caribbean Javanese",
      "hvn": "Sabu",
      "nij": "Ngaju",
      "dwr": "Dawro",
      "izz": "Izii",
      "msm": "Agusan Manobo",
      "bus": "Bokobaru",
      "ktu": "Kituba",
      "chr": "Cherokee",
      "maz": "Central Mazahua",
      "tzj": "Tz'utujil",
      "suz": "Sunwar",
      "knj": "Western Kanjobal",
      "bim": "Bimoba",
      "gvl": "Gulay",
      "bqc": "Boko",
      "tca": "Ticuna",
      "pis": "Pijin",
      "prk": "Parauk",
      "laj": "Lango",
      "mel": "Central Melanau",
      "qxr": "Cañar Highland Quichua",
      "niq": "Nandi",
      "ahk": "Akha",
      "shp": "Shipibo-Conibo",
      "hne": "Chhattisgarhi",
      "spp": "Supyire Senoufo",
      "koi": "Komi-Permyak",
      "krj": "Kinaray-a",
      "quf": "Lambayeque Quechua",
      "luz": "Southern Luri",
      "agr": "Aguaruna",
      "tsc": "Tswa",
      "mqy": "Manggarai",
      "gof": "Gofa",
      "gbm": "Garhwali",
      "miq": "Mískito",
      "dje": "Zarma",
      "awa": "Awadhi",
      "bjj": "Kanauji",
      "qvz": "Quichua",
      "sjp": "Surjapuri",
      "tll": "Tetela",
      "raj": "Rajasthani",
      "kjg": "Khmu",
      "bgz": "Banggai",
      "quy": "Ayacucho Quechua",
      "cbk": "Chavacano",
      "akb": "Batak Angkola",
      "oj": "Ojibwa",
      "ify": "Kallahan",
      "mey": "Hassaniyya",
      "ks": "Kashmiri",
      "cac": "Chuj",
      "brx": "Bodo",
      "qup": "Southern Pastaza Quechua",
      "syl": "Sylheti",
      "jax": "Jambi Malay",
      "ff": "Fulah",
      "ber": "Berber",
      "tks": "Takestani",
      "trp": "Kok Borok",
      "mrw": "Maranao",
      "adh": "Adhola",
      "smt": "Simte",
      "srr": "Serer",
      "ffm": "Maasina Fulfulde",
      "qvc": "Cajamarca Quechua",
      "mtr": "Mewari",
      "ann": "Obolo",
      "aa": "Afar",
      "noe": "Nimadi",
      "nut": "Nung",
      "gyn": "Guyanese Creole English",
      "kwi": "Awa-Cuaiquer",
      "xmm": "Manado Malay",
      "msb": "Masbatenyo"
      },
      "pipeline_tag": "translation",    
      "model_desc": "MADLAD-400-3B-MT is a multilingual machine translation model based on the T5 architecture that was trained on 1 trillion tokens covering over 450 languages using publicly available data. It is competitive with models that are significantly larger.",
      "model_card_url": "https://huggingface.co/google/madlad400-3b-mt",
      "requirements": {
        "required_classes": {
          "model": "AutoModelForSeq2SeqLM",
          "tokenizer": "AutoTokenizer"
        }
      }
  },
  "facebook/nllb-200-distilled-600M": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "Translation",
      "Multilingual"
    ],
    "languages": {
      "ace_Arab": "Achinese Arabic",
      "ace_Latn": "Achinese Latin",
      "acm_Arab": "Mesopotamian Arabic",
      "acq_Arab": "Ta'izzi-Adeni Arabic",
      "aeb_Arab": "Tunisian Arabic",
      "afr_Latn": "Afrikaans",
      "ajp_Arab": "South Levantine Arabic",
      "aka_Latn": "Akan",
      "amh_Ethi": "Amharic",
      "apc_Arab": "North Levantine Arabic",
      "arb_Arab": "Standard Arabic",
      "ars_Arab": "Najdi Arabic",
      "ary_Arab": "Moroccan Arabic",
      "arz_Arab": "Egyptian Arabic",
      "asm_Beng": "Assamese",
      "ast_Latn": "Asturian",
      "awa_Deva": "Awadhi",
      "ayr_Latn": "Central Aymara",
      "azb_Arab": "South Azerbaijani",
      "azj_Latn": "North Azerbaijani",
      "bak_Cyrl": "Bashkir",
      "bam_Latn": "Bambara",
      "ban_Latn": "Balinese",
      "bel_Cyrl": "Belarusian",
      "bem_Latn": "Bemba",
      "ben_Beng": "Bengali",
      "bho_Deva": "Bhojpuri",
      "bjn_Arab": "Banjar Arabic",
      "bjn_Latn": "Banjar Latin",
      "bod_Tibt": "Standard Tibetan",
      "bos_Latn": "Bosnian",
      "bug_Latn": "Buginese",
      "bul_Cyrl": "Bulgarian",
      "cat_Latn": "Catalan",
      "ceb_Latn": "Cebuano",
      "ces_Latn": "Czech",
      "cjk_Latn": "Chokwe",
      "ckb_Arab": "Central Kurdish",
      "crh_Latn": "Crimean Tatar",
      "cym_Latn": "Welsh",
      "dan_Latn": "Danish",
      "deu_Latn": "German",
      "dik_Latn": "Southwestern Dinka",
      "dyu_Latn": "Dyula",
      "dzo_Tibt": "Dzongkha",
      "ell_Grek": "Modern Greek",
      "eng_Latn": "English",
      "epo_Latn": "Esperanto",
      "est_Latn": "Estonian",
      "eus_Latn": "Basque",
      "ewe_Latn": "Ewe",
      "fao_Latn": "Faroese",
      "pes_Arab": "Western Persian",
      "fij_Latn": "Fijian",
      "fin_Latn": "Finnish",
      "fon_Latn": "Fon",
      "fra_Latn": "French",
      "fur_Latn": "Friulian",
      "fuv_Latn": "Nigerian Fulfulde",
      "gla_Latn": "Scottish Gaelic",
      "gle_Latn": "Irish",
      "glg_Latn": "Galician",
      "grn_Latn": "Guarani",
      "guj_Gujr": "Gujarati",
      "hat_Latn": "Haitian Creole",
      "hau_Latn": "Hausa",
      "heb_Hebr": "Hebrew",
      "hin_Deva": "Hindi",
      "hne_Deva": "Chhattisgarhi",
      "hrv_Latn": "Croatian",
      "hun_Latn": "Hungarian",
      "hye_Armn": "Armenian",
      "ibo_Latn": "Igbo",
      "ilo_Latn": "Ilocano",
      "ind_Latn": "Indonesian",
      "isl_Latn": "Icelandic",
      "ita_Latn": "Italian",
      "jav_Latn": "Javanese",
      "jpn_Jpan": "Japanese",
      "kab_Latn": "Kabyle",
      "kac_Latn": "Jingpho",
      "kam_Latn": "Kamba",
      "kan_Knda": "Kannada",
      "kas_Arab": "Kashmiri Arabic",
      "kas_Deva": "Kashmiri Devanagari",
      "kat_Geor": "Georgian",
      "knc_Arab": "Central Kanuri Arabic",
      "knc_Latn": "Central Kanuri Latin",
      "kaz_Cyrl": "Kazakh",
      "kbp_Latn": "Kabiyè",
      "kea_Latn": "Kabuverdianu",
      "khm_Khmr": "Khmer",
      "kik_Latn": "Kikuyu",
      "kin_Latn": "Kinyarwanda",
      "kir_Cyrl": "Kyrgyz",
      "kmb_Latn": "Kimbundu",
      "kon_Latn": "Kikongo",
      "kor_Hang": "Korean",
      "kmr_Latn": "Northern Kurdish",
      "lao_Laoo": "Lao",
      "lvs_Latn": "Standard Latvian",
      "lij_Latn": "Ligurian",
      "lim_Latn": "Limburgish",
      "lin_Latn": "Lingala",
      "lit_Latn": "Lithuanian",
      "lmo_Latn": "Lombard",
      "ltg_Latn": "Latgalian",
      "ltz_Latn": "Luxembourgish",
      "lua_Latn": "Luba-Kasai",
      "lug_Latn": "Ganda",
      "luo_Latn": "Luo",
      "lus_Latn": "Mizo",
      "mag_Deva": "Magahi",
      "mai_Deva": "Maithili",
      "mal_Mlym": "Malayalam",
      "mar_Deva": "Marathi",
      "min_Latn": "Minangkabau",
      "mkd_Cyrl": "Macedonian",
      "plt_Latn": "Plateau Malagasy",
      "mlt_Latn": "Maltese",
      "mni_Beng": "Meitei",
      "khk_Cyrl": "Halh Mongolian",
      "mos_Latn": "Mossi",
      "mri_Latn": "Maori",
      "zsm_Latn": "Standard Malay",
      "mya_Mymr": "Burmese",
      "nld_Latn": "Dutch",
      "nno_Latn": "Norwegian Nynorsk",
      "nob_Latn": "Norwegian Bokmål",
      "npi_Deva": "Nepali",
      "nso_Latn": "Northern Sotho",
      "nus_Latn": "Nuer",
      "nya_Latn": "Nyanja",
      "oci_Latn": "Occitan",
      "gaz_Latn": "West Central Oromo",
      "ory_Orya": "Odia",
      "pag_Latn": "Pangasinan",
      "pan_Guru": "Eastern Panjabi",
      "pap_Latn": "Papiamento",
      "pol_Latn": "Polish",
      "por_Latn": "Portuguese",
      "prs_Arab": "Dari",
      "pbt_Arab": "Southern Pashto",
      "quy_Latn": "Ayacucho Quechua",
      "ron_Latn": "Romanian",
      "run_Latn": "Rundi",
      "rus_Cyrl": "Russian",
      "sag_Latn": "Sango",
      "san_Deva": "Sanskrit",
      "sat_Beng": "Santali",
      "scn_Latn": "Sicilian",
      "shn_Mymr": "Shan",
      "sin_Sinh": "Sinhala",
      "slk_Latn": "Slovak",
      "slv_Latn": "Slovenian",
      "smo_Latn": "Samoan",
      "sna_Latn": "Shona",
      "snd_Arab": "Sindhi",
      "som_Latn": "Somali",
      "sot_Latn": "Southern Sotho",
      "spa_Latn": "Spanish",
      "als_Latn": "Tosk Albanian",
      "srd_Latn": "Sardinian",
      "srp_Cyrl": "Serbian",
      "ssw_Latn": "Swati",
      "sun_Latn": "Sundanese",
      "swe_Latn": "Swedish",
      "swh_Latn": "Swahili",
      "szl_Latn": "Silesian",
      "tam_Taml": "Tamil",
      "tat_Cyrl": "Tatar",
      "tel_Telu": "Telugu",
      "tgk_Cyrl": "Tajik",
      "tgl_Latn": "Tagalog",
      "tha_Thai": "Thai",
      "tir_Ethi": "Tigrinya",
      "taq_Latn": "Tamasheq Latin",
      "taq_Tfng": "Tamasheq Tifinagh",
      "tpi_Latn": "Tok Pisin",
      "tsn_Latn": "Tswana",
      "tso_Latn": "Tsonga",
      "tuk_Latn": "Turkmen",
      "tum_Latn": "Tumbuka",
      "tur_Latn": "Turkish",
      "twi_Latn": "Twi",
      "tzm_Tfng": "Central Atlas Tamazight",
      "uig_Arab": "Uyghur",
      "ukr_Cyrl": "Ukrainian",
      "umb_Latn": "Umbundu",
      "urd_Arab": "Urdu",
      "uzn_Latn": "Northern Uzbek",
      "vec_Latn": "Venetian",
      "vie_Latn": "Vietnamese",
      "war_Latn": "Waray",
      "wol_Latn": "Wolof",
      "xho_Latn": "Xhosa",
      "ydd_Hebr": "Eastern Yiddish",
      "yor_Latn": "Yoruba",
      "yue_Hant": "Yue Chinese",
      "zho_Hans": "Chinese Simplified",
      "zho_Hant": "Chinese Traditional",
      "zul_Latn": "Zulu"
    },
    "pipeline_tag": "translation",    
    "model_desc": "NLLB-200 is a machine translation model primarily intended for research in machine translation, - especially for low-resource languages. It allows for single sentence translation among 200 languages. Information on how to - use the model can be found in Fairseq code repository along with the training code and references to evaluation and training data. Primary users are researchers and machine translation research community. NLLB-200 is trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation. The model was trained with input lengths not exceeding 512 tokens, therefore translating longer sequences might result in quality degradation. NLLB-200 translations can not be used as certified translations.",
    "model_card_url": "https://huggingface.co/facebook/nllb-200-distilled-600M",
    "requirements": {
      "required_classes": {
        "model": "AutoModelForSeq2SeqLM",
        "tokenizer": "AutoTokenizer"
      }
    },
    "config": {
      "pipeline_config": {
        "src_lang": "eng_Latn",
        "tgt_lang": "por_Latn"
      }
    }
  },
  "openai/whisper-large-v3": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "Automatic Speech Recognition",
      "Speech to Text",
      "Audio",
      "Multilingual"
    ],
    "languages": {
      "en": "English",
      "zh": "Chinese",
      "de": "German",
      "es": "Spanish",
      "ru": "Russian",
      "ko": "Korean",
      "fr": "French",
      "ja": "Japanese",
      "pt": "Portuguese",
      "tr": "Turkish",
      "pl": "Polish",
      "ca": "Catalan",
      "nl": "Dutch",
      "ar": "Arabic",
      "sv": "Swedish",
      "it": "Italian",
      "id": "Indonesian",
      "hi": "Hindi",
      "fi": "Finnish",
      "vi": "Vietnamese",
      "he": "Hebrew",
      "uk": "Ukrainian",
      "el": "Greek",
      "ms": "Malay",
      "cs": "Czech",
      "ro": "Romanian",
      "da": "Danish",
      "hu": "Hungarian",
      "ta": "Tamil",
      "no": "Norwegian",
      "th": "Thai",
      "ur": "Urdu",
      "hr": "Croatian",
      "bg": "Bulgarian",
      "lt": "Lithuanian",
      "la": "Latin",
      "mi": "Maori",
      "ml": "Malayalam",
      "cy": "Welsh",
      "sk": "Slovak",
      "te": "Telugu",
      "fa": "Persian",
      "lv": "Latvian",
      "bn": "Bengali",
      "sr": "Serbian",
      "az": "Azerbaijani",
      "sl": "Slovenian",
      "kn": "Kannada",
      "et": "Estonian",
      "mk": "Macedonian",
      "br": "Breton",
      "eu": "Basque",
      "is": "Icelandic",
      "hy": "Armenian",
      "ne": "Nepali",
      "mn": "Mongolian",
      "bs": "Bosnian",
      "kk": "Kazakh",
      "sq": "Albanian",
      "sw": "Swahili",
      "gl": "Galician",
      "mr": "Marathi",
      "pa": "Punjabi",
      "si": "Sinhala",
      "km": "Khmer",
      "sn": "Shona",
      "yo": "Yoruba",
      "so": "Somali",
      "af": "Afrikaans",
      "oc": "Occitan",
      "ka": "Georgian",
      "be": "Belarusian",
      "tg": "Tajik",
      "sd": "Sindhi",
      "gu": "Gujarati",
      "am": "Amharic",
      "yi": "Yiddish",
      "lo": "Lao",
      "uz": "Uzbek",
      "fo": "Faroese",
      "ht": "Haitian Creole",
      "ps": "Pashto",
      "tk": "Turkmen",
      "nn": "Norwegian Nynorsk",
      "mt": "Maltese",
      "sa": "Sanskrit",
      "lb": "Luxembourgish",
      "my": "Burmese",
      "bo": "Tibetan",
      "tl": "Tagalog",
      "mg": "Malagasy",
      "as": "Assamese",
      "tt": "Tatar",
      "haw": "Hawaiian",
      "ln": "Lingala",
      "ha": "Hausa",
      "ba": "Bashkir",
      "jw": "Javanese",
      "su": "Sundanese"
    },
    "pipeline_tag": "automatic-speech-recognition",
    "model_desc": "Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning. Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford et al. from OpenAI. The original code repository can be found here. Whisper large-v3 has the same architecture as the previous large models except the following minor differences: The input uses 128 Mel frequency bins instead of 80 A new language token for Cantonese The Whisper large-v3 model is trained on 1 million hours of weakly labeled audio and 4 million hours of pseudolabeled audio collected using Whisper large-v2. The model was trained for 2.0 epochs over this mixture dataset. The large-v3 model shows improved performance over a wide variety of languages, showing 10% to 20% reduction of errors compared to Whisper large-v2.",
    "model_card_url": "https://huggingface.co/openai/whisper-large-v3",
    "requirements": {
      "required_classes": {
        "model": "AutoModelForSpeechSeq2Seq",
        "processor": "AutoProcessor"
      }
    },
    "config": {
      "pipeline_config": {
        "return_timestamps": false, 
        "generate_kwargs": {
          "language": null, 
          "task": null
        }
      }
    }
  },
  "meta-llama/Meta-Llama-3.1-8B-Instruct": {
    "is_online": false,
    "model_source": "transformers",
    "model_class": "TransformerModel",
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "llama",
      "meta",
      "llama-3",
      "text-generation",
      "text-generation-inference",
      "en",
      "license:llama3",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "model_card_url": "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct",
    "requirements": {
      "required_classes": {
        "model": "AutoModelForCausalLM",
        "tokenizer": "AutoTokenizer"
      },
      "requires_auth": true
    },
    "model_desc": "Meta-Llama-3.1-8B-Instruct is part of the Meta Llama 3 family of large language models, optimized for instruction-following tasks. It's an 8B parameter model fine-tuned on a high-quality instruction dataset, making it particularly suitable for conversational AI and task-oriented applications.",
    "model_detail": "https://llama.meta.com/get-started/",
    "config": {
      "model_config": {
        "torch_dtype": "bfloat16"
      },
      "tokenizer_config": {},
      "processor_config": {},
      "pipeline_config": {
        "max_length": 512,
        "max_new_tokens": 1000,
        "num_beams": 2,
        "use_cache": true
      },
      "device_config": {
        "device": "cuda"
      }
    }
  }
}